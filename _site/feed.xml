<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rahul Roy&apos;s Portfolio</title>
    <description>Northwestern MS in Robotics</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 12 Dec 2023 20:23:13 -0600</pubDate>
    <lastBuildDate>Tue, 12 Dec 2023 20:23:13 -0600</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>BotChocolate</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, ROS2, Python, Motion Planning, Moveit, Intel Realsense, Emika Franka Robot Arm&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/Q_aNWWe4h5M&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The purpose of this project was to create hot chocolate using the Franka Emika 7 DOF robot arm. To perceive its environment, the system utilizes an Intel D435i camera AprilTags. Upon initial setup, a calibration process must be completed. After this, the process consisted of using AprilTags to locate a scoop of cocoa, a mug, a spoon, and a kettle relative to the robot. Next, using our custom MoveIt API for Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;movebot&lt;/code&gt;, the robot is able to perform path planning between all of these objects. It turns on the kettle, dumps the cocoa powder into the mug, pours the hot water over the power, and stirs the mixture it with the spoon.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/bot_chocolate&quot;&gt;View BotChocolate Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;moveit-api&quot;&gt;Moveit API&lt;/h2&gt;
&lt;p&gt;Because the ROS2 MoveIt package does not have a Python API yet, creating one was the first step. By 
looking into the MoveIt ROS2 actions and services, we were able to create an API that takes either 
Cartesian coordinates of the Franka end-efffector or joint-state positions of each joint and creates
a path to the specified position. It also allows for the choice of moving in a Cartesian path.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Sed9XwHT-7c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;
&lt;p&gt;The vision system is comprised of two major parts: calibration and component location. AprilTags are
used to detect where each of the hot chocolate making components are. The AprilTags are located using
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltag_ros&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;There is one tag fixed to a kettle and one tag fixed to a jig that has slots for a mug, cocoa scooper,
and a spoon. Once these tags are found in the camera frame, there are two transformation trees 
describing where all the hot chocolate components are relative to the camera and where each link of
the Franka robot arm is relative to the robot’s base. However, we need to connect these trees to 
get the location of each component relative to the robot’s base. This is where the calibration is 
used. An AprilTag must be placed in the robot’s gripper, aligned with the gripper’s
coordinate frame and in view of the camera to calibrate. We then wrote a program that finds the location of the tag
and relates it to the position of the base, connecting the robot frame to the camera frame. We then
save this transformation to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.yaml&lt;/code&gt; file to be used later on when running the main hot chocolate
making sequence.&lt;/p&gt;

&lt;p&gt;After obtaining the transformation from the camera to the robot, the robot can now use the locations
of the two AprilTags to locate and manipulate the hot chocolate components as necessary.&lt;/p&gt;

&lt;h2 id=&quot;motion-planning&quot;&gt;Motion Planning&lt;/h2&gt;
&lt;p&gt;Once the locations of all the components are known, it’s time to make hot chocolate. Cartesian paths
are used to move the robot in straight lines and rotational path planning is used for achieving motions
like tilting to grab the scoop and dumping the cocoa. Both Cartesian and rotational motion are used to achieve pouring.
To make a cup of hot chocolate, the robot first turns on the kettle to heat the water, grab the cocoa scoop, 
pours it in a mug, pours the hot water over the powder, and stirs. We also used serval intermediate 
waypoints to help avoid reaching the joint limits of the robot.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Shantao Cao&lt;/li&gt;
  &lt;li&gt;Allan Garcia-Casal&lt;/li&gt;
  &lt;li&gt;Nicholas Marks&lt;/li&gt;
  &lt;li&gt;James Oubre&lt;/li&gt;
  &lt;li&gt;David Dorf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/206768445-4503edc2-2075-48b4-baf7-e6dc7bd3ca86.png&quot; alt=&quot;bot_choc-min&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Botchoc/</link>
        <guid isPermaLink="true">http://localhost:4000/Botchoc/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>ROS2</category>
        
        <category>Python</category>
        
        <category>Motion Planning</category>
        
        <category>Moveit</category>
        
        <category>Intel Realsense</category>
        
        <category>Emika Franka Robot Arm</category>
        
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;Programmed a Pincher X100 4-DOF robot arm to grasp a purple colored pen.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://youtube.com/embed/iNhg6hstLbM?feature=share&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;I utilized an Intel Realsense D435i camera to locate a purple Northwestern pen and manipulate it using a Trossen PincherX 100 robot arm. To identify the pen’s position, I transformed the RGB image from the Realsense into an HSV image and employed the HSV values to identify all purple-hued pixels. Following this, I established a binary map where the identified pixels were depicted as white and all other pixels as black. Utilizing OpenCV’s contour detection, I determined the pixel coordinates of the pen’s centroid. This enabled me to ascertain the pen’s position in relation to the camera by utilizing the aligned depth image generated by the Realsense. Subsequently, I converted the pen’s position from the camera’s frame to that of the frame of the robot and directed the robot to maneuver towards the pen and seize it.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt; &lt;a href=&quot;https://github.com/roy2909/pen_challenge&quot;&gt;View it on Github&lt;/a&gt;&lt;/h4&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 15 Sep 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/Pen/</link>
        <guid isPermaLink="true">http://localhost:4000/Pen/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>PincherX 100</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
  </channel>
</rss>
