<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rahul Roy&apos;s Portfolio</title>
    <description>MS in Robotics - Northwestern University</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 17 Dec 2023 22:00:08 -0600</pubDate>
    <lastBuildDate>Sun, 17 Dec 2023 22:00:08 -0600</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>CoppeliaSim Youbot Pick and Place simulation</title>
        <description>&lt;p&gt;Simluation of  youBot mobile manipulator in Coppeliasim picking up a cube from one location and placing it down at a final position&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/DZvI7udHxDo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This software plans a trajectory of the end effector of the youBot mobile manipulator (a mobile base with four mecanum wheels and a 5R robot arm), to pick up a block from a specified location, carry it to a final location and place it down. It also performs odometry as the chassis moves and feedback control to drive the youBot to pick up the block. The final output of the software will be a comma-separated values (csv) text file that specifies the configurations of the chassis and the arm, the angles of the four wheels, and the state of the gripper (open or closed) as a function of time. The initial configuration (the chassis phi, chassis x, chassis y, J1, J2, J3, J4, J5, W1, W2, W3, W4, gripper state = 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), used is (x, y, ùúÉ) = (0.2, 0, 0). The initial block configuration is at (x, y, ùúÉ) = (1m, 0m, 0 rad) and the final block configuration is at (x, y, ùúÉ) = (0 m,-1m, -œÄ/2 rad). In ‚ÄòNew Task‚Äô, the initial block configuration is at at (x, y, ùúÉ) = (0.5m, 1.0m, 0 rad) and the final block configuration is at (x, y, ùúÉ) = (0.5m, -1.5m, -œÄ/2rad). The results were obtained using a feedforward PI controller tuned individually for each task.&lt;/p&gt;

&lt;h2 id=&quot;algorithm-description&quot;&gt;Algorithm description&lt;/h2&gt;
&lt;p&gt;The NextState function employs the first-order Euler method to determine the robot‚Äôs configuration at the next
time step.&lt;/p&gt;

&lt;p&gt;The TrajectoryGenerator function is responsible for generating the reference trajectory for the end-effector frame {e}. This trajectory consists of eight concatenated trajectory segments, as described below. Each trajectory segment begins and ends 
at rest.&lt;/p&gt;

&lt;p&gt;Across 8 stages in the simulation, the process unfolds as
follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A trajectory to move the gripper from its initial configuration to a ‚Äústandoff‚Äù configuration a few cm
above the block.&lt;/li&gt;
  &lt;li&gt;A trajectory to move the gripper down to the grasp position.&lt;/li&gt;
  &lt;li&gt;Closing of the gripper.&lt;/li&gt;
  &lt;li&gt;A trajectory to move the gripper back up to the ‚Äústandoff‚Äù configuration.&lt;/li&gt;
  &lt;li&gt;A trajectory to move the gripper to a ‚Äústandoff‚Äù configuration above the final configuration.&lt;/li&gt;
  &lt;li&gt;A trajectory to move the gripper to the final configuration of the object.&lt;/li&gt;
  &lt;li&gt;Opening of the gripper.&lt;/li&gt;
  &lt;li&gt;A trajectory to move the gripper back to the ‚Äústandoff‚Äù configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Throughout each stage, the algorithm uses the ScrewTrajectory function from the modern robotics python library. This function, utilizing the third-order or fifth-order polynomial time-scaling method, takes the start and end transformations of the gripper as inputs. It generates a discrete trajectory in the form of a list of end-effector transformation matrices.
The FeedbackControl function is instrumental in computing the kinematic task-space feedforward plus feedback control law.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/ccd0415dcf0def9e3a0e2c0697e6d3649bcf640c/assets/images/feedbacklaw.png&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;div align=&quot;left&quot;&gt;&lt;h4&gt; Best &lt;/h4&gt;&lt;/div&gt;
&lt;p&gt;The best results were obtained by tuning the proportional gain(Kp) to 3.0 and the integral gain(Ki) to 0.01. As seen from the error plot there is a tiny error in the middle but converges after that. The initial block configuration is at (x, y, ùúÉ) = (1m, 0m, 0 rad) and the final block configuration is at (x, y, ùúÉ) = (0 m,-1m, -œÄ/2 rad).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/70ef4d72319e78af4889d3be62dcb52665a20acb/assets/images/best.png&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;div align=&quot;left&quot;&gt;&lt;h4&gt; Overshoot &lt;/h4&gt;&lt;/div&gt;
&lt;p&gt;The overshoot results were obtained by tuning the proportional gain(Kp) to 2.0 and the integral gain(Ki) to 0.01. As seen from the error plot there is a tiny error in the middle but converges after that. It overshoots slightly in the beginning as seen from the red loop but converges after that. The initial block configuration is at (x, y, ùúÉ) = (1m, 0m, 0 rad) and the final block configuration is at (x, y, ùúÉ) = (0 m,-1m, -œÄ/2 rad).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/70ef4d72319e78af4889d3be62dcb52665a20acb/assets/images/overshoot.png&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;div align=&quot;left&quot;&gt;&lt;h4&gt; newTask &lt;/h4&gt;&lt;/div&gt;
&lt;p&gt;The newTask results were obtained by tuning the proportional gain(Kp) to 2.0 and the integral gain(Ki) to 0.01. As seen from the error plot there is a tiny error in the middle but converges after that. The initial block configuration is now at (x, y, ùúÉ) = (0.5m, 1.0m, 0 rad) and the final block configuration is at (x, y, ùúÉ) = (0.5 m,-1.5m, -œÄ/2 rad).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/70ef4d72319e78af4889d3be62dcb52665a20acb/assets/images/newTask.png&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;¬†&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;h4&gt; &lt;a href=&quot;https://github.com/roy2909/KukaYouBot/tree/main&quot;&gt;View Project on Github&lt;/a&gt;&lt;/h4&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 07 Dec 2023 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Kuka/</link>
        <guid isPermaLink="true">http://localhost:4000/Kuka/</guid>
        
        
        <category>Mobile Manipulation</category>
        
        <category>CoppeliaSim</category>
        
        <category>PID Control</category>
        
        <category>Python</category>
        
      </item>
    
      <item>
        <title>Motion Controlled Differential Drive Car with Infrared Sensor (IR) Operated Gripper</title>
        <description>&lt;p&gt;Programmed a nRF52833 microcontroller to control a differential drive car and autonomously operate a gripper on detection of objects using an IR sensor&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of Northwestern‚Äôs microprocessor design course I took in the fall, my group and I created a motion controlled differential drive car which autonomously opens and closes a gripper on detection of objects (heat sources).&lt;/p&gt;

&lt;p&gt;Car in action&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/335c33a8b59c644fb7b8f54dcdef2fb14fb2cdd6/assets/images/grip.gif&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;¬†&lt;/p&gt;

&lt;h2 id=&quot;system-and-control&quot;&gt;System and Control&lt;/h2&gt;

&lt;p&gt;The system&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/9e7efdc4fe540ea58a439d714bdc78164ebd8469/assets/images/fd.png&quot; alt=&quot;car_setup&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;¬†&lt;/p&gt;

&lt;p&gt;The system comprises various components working in tandem: two microcontrollers communicating via radio - the BBC Micro:bit V2 (nRF52833) and the Motobit, responsible for motor control in the RC car (utilizing the SparkFun moto:bit - micro:bit Carrier Board). Additionally, the system includes a PWM-controlled gripper (servo - MG996R) and an IR sensor (SparkFun Grid-EYE Infrared Array Breakout).&lt;/p&gt;

&lt;p&gt;Developed around the Micro:bit V2, the project involved coding directly into the Micro:bit‚Äôs microcontroller (nRF52833) using C programming language. Leveraging two sensors, namely the built-in accelerometer (for which a custom Tilt driver was written to provide Euler angles to the Micro:bit controlling the car base via radio communication) and the IR sensor (Grid-EYE), the system is designed for robust functionality.&lt;/p&gt;

&lt;p&gt;Communication between the microcontrollers is facilitated through Radio for which a wireless communication driver was written, allowing the transmission of Euler angles from the accelerometer-equipped Micro:bit to the car-based Micro:bit. This data dictates the movement of the RC car via the motodriver, controlled using I2C communication.&lt;/p&gt;

&lt;p&gt;The end-effector, a mechanical gripper, operates in response to a servo (MG996R) controlled by a Micro:bit that sends Pulse Width Modulation (PWM) signals via I2C. Triggered by the breakout IR Grid-EYE sensor‚Äôs heat signatures, the gripper engages and disengages through a stem attachment.&lt;/p&gt;

&lt;p&gt;The Car being controlled by the Microbit&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/6636d5643b15a614417a1c87e04d364e50d60b3b/assets/images/car1.gif&quot; alt=&quot;car_gif&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;¬†&lt;/p&gt;

&lt;h2 id=&quot;operation&quot;&gt;Operation&lt;/h2&gt;

&lt;p&gt;The system operates as follows. It begins with the user tilting the Micro:bit, operating the Tilt driver for the accelerometer. This action generates precise Euler angles, transmitted as data packets via Radio communication to another Micro:bit positioned on the car‚Äôs base.&lt;/p&gt;

&lt;p&gt;The Micro:bit on the car base serves as the command center, engaging in two primary functionalities. Firstly, it communicates via the I2C protocol with the motodriver, responsible for controlling the car‚Äôs motors. This communication translates the received Euler angles into specific motor commands, directing the car‚Äôs movement based on both the tilt direction and the degree of inclination.&lt;/p&gt;

&lt;p&gt;Simultaneously, the system monitors its surroundings using the breakout IR Grid-EYE sensor. Upon detecting a heat signature, indicative of an object, this information is relayed back to the Micro:bit attached to the motodriver.&lt;/p&gt;

&lt;p&gt;The Micro:bit interprets this data and generates Pulse Width Modulation (PWM) signals, which travel over the I2C protocol to the servo responsible for the gripper mechanism. These signals precisely control the opening and closing actions of the gripper, enabling it to interact with and manipulate objects within its reach.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Graham Clifford&lt;/li&gt;
  &lt;li&gt;Rahul Roy
¬†&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&quot;center&quot;&gt;&lt;h4&gt; &lt;a href=&quot;https://github.com/roy2909/Motioncar&quot;&gt;View Project on Github&lt;/a&gt;&lt;/h4&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 05 Dec 2023 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Car/</link>
        <guid isPermaLink="true">http://localhost:4000/Car/</guid>
        
        
        <category>C</category>
        
        <category>Mechatronics</category>
        
        <category>Embedded Systems</category>
        
        <category>I2C</category>
        
        <category>nRF52833</category>
        
        <category>PWM</category>
        
      </item>
    
      <item>
        <title>Autonomous Colored Bowling Pin Targeting with Franka Arm and Nerf Blaster</title>
        <description>&lt;p&gt;Programmed a 7 DOF Emika Franka Arm to detect and knock down colored bowling pins.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/OQsgt1Qhr0Y&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this project, the Franka arm scans the environment for randomly placed bowling pins of known colors(blue, green, red, yellow) and knocks them down. Mounted with an Intel Realsense D435i depth camera, it perceptively observes the environment, while employing a Nerf blaster to accurately aim and knock over the identified targets. The user has the option to specify the initial color of the bowling pins for the arm to knock over. Once all the selected colored pins are knocked down, the arm pauses, awaiting a subsequent input to resume targeting differently colored pins. Additionally, it possesses the capability to switch between two Nerf blasters if ammunition runs low during the operation.&lt;/p&gt;

&lt;h2 id=&quot;control-flow-and-nodes&quot;&gt;Control Flow and Nodes&lt;/h2&gt;
&lt;p&gt;Control: 
Main node which is used to call services to other nodes to carry out the project.&lt;/p&gt;

&lt;p&gt;Shoot: 
Node that carries out all moveit-interface services such as cartesian planning, IK planning, and gripper requests.&lt;/p&gt;

&lt;p&gt;Yolo: 
Node that runs YOLO (You only look once) to find the colored bowling pins with respect to the base of the Franka arm and display them as colored markers in Rviz2.&lt;/p&gt;

&lt;p&gt;User_Input: 
Node that listens to the user‚Äôs audio input to set the color of the targeted pins.&lt;/p&gt;

&lt;p&gt;Trigger: 
Node that controls the Arduino for the gun‚Äôs trigger.&lt;/p&gt;

&lt;p&gt;Apriltag_node: 
Node that scans and gives the coordinates for the April tags&lt;/p&gt;

&lt;p&gt;The control flow diagram is shown below:&lt;/p&gt;

&lt;p class=&quot;mb-5&quot;&gt;&lt;img class=&quot;shadow-lg&quot; src=&quot;/assets/images/Blank diagram.png&quot; alt=&quot;Control Flow Diagram&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;user-input&quot;&gt;User Input&lt;/h2&gt;
&lt;p&gt;The user input is done using pyaudio package with the Google Web Speech API that listens to an user‚Äôs choice of target and transcibes that to a string that can be used in aiming at the chosen target. The inputs are set to only red, blue, green, and yellow.&lt;/p&gt;

&lt;h2 id=&quot;image-pipeline&quot;&gt;Image Pipeline&lt;/h2&gt;
&lt;p&gt;The Image pipeline is comprised of two major parts: Scanning(Object detection and classification) and AprilTags. 
Object detection and classification is done with the help of the YOLOv8(You Only Look Once) model which is a
real-time object detection and image segmentation model. The model was trained on more than 300 images of five classes:
blue_pins, red_pins, yellow_pins, green_pins  and not_pins. This was used to detect the colored bowling pins in the environment. With the help of the Intel Realsense D435i depth camera mounted on top of the Franka‚Äôs arm, the depth of the 
pins as well as the x, y coordinates were calculated with respect to the Franka‚Äôs base and this was published as Markers in Rviz2&lt;/p&gt;

&lt;p&gt;AprilTags are used to detect the position of the Nerf blasters in order to pick them up once the pin scanning was done. The AprilTags are located using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltag_ros&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;Shown below are the blue and yellow pins published as colored markers in Rviz2&lt;/p&gt;
&lt;p class=&quot;mb-5&quot;&gt;&lt;img class=&quot;shadow-lg&quot; src=&quot;/assets/images/rviz.png&quot; alt=&quot;Rviz markers&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;motion-planning&quot;&gt;Motion Planning&lt;/h2&gt;
&lt;p&gt;Motion Planning is done using The MoveItApi - a python wrapper for basic MoveIt functionality offered by the MoveIt ROS API.
It used Cartesian Planning to pick up the blasters and Inverse Kinematics for the scanning positions.&lt;/p&gt;

&lt;h2 id=&quot;control&quot;&gt;Control&lt;/h2&gt;
&lt;p&gt;The main control node is used to integrate all functionalities of the system together. It starts with  the User input to select which pin gets knocked down. Scanning of the bowling pins is next, calling a the YOLO node to detect and publish markers of the detected pins. Once the pins are detected, the nerf blasters are loacted and picked up using the gun_pickup service and then targets are aimed at. The final service call is for the trigger mechanism that triggers the nerf blaster to fire at the aimed target.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future Work&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;One of the biggest issues is the success rate of hitting targets because even if the gun is correctly pointed at the target, the error from the gun can cause the gun to miss. This would be something that would ideally be fixed by either increasing the accuracy of the gun so that as long as the gun is properly aimed that it would hit the target or increasing the bullets to increase the chances of hitting. Another option would be to include motion while the gun is shooting to try to correct the error from the gun.&lt;/li&gt;
  &lt;li&gt;360-degree scanning and shooting. Currently, the arm is only able to scan and shoot targets in front of itself so the next step would be adding the ability to target in any direction. Additionally, further testing on the limits of the height of the targets would also be necessary to determine how high and how low a target can be and have the arm still successfully shoot it.&lt;/li&gt;
  &lt;li&gt;Dynamic aiming. The assumption is made that the targets are stationary and unable to move, so being able to add a camera to allow for the arm to constantly scan for targets would allow for two different things. The first is a dynamic environment where targets are moved before the gun is shot and remain stationary and can be hit, and the second is moving targets where the gun can track its motion and shoot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Rahul Roy(YOLO, Computer vision)&lt;/li&gt;
  &lt;li&gt;Maximiliano Palay&lt;/li&gt;
  &lt;li&gt;Sophia Schiffer&lt;/li&gt;
  &lt;li&gt;Jialu Yu&lt;/li&gt;
  &lt;li&gt;Joel Goh&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt; &lt;a href=&quot;https://github.com/roy2909/TargetBot&quot;&gt;View Project on Github&lt;/a&gt;&lt;/h4&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 04 Dec 2023 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Nerf/</link>
        <guid isPermaLink="true">http://localhost:4000/Nerf/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>ROS2</category>
        
        <category>Python</category>
        
        <category>Motion Planning</category>
        
        <category>Moveit</category>
        
        <category>Intel Realsense</category>
        
        <category>Emika Franka Robot Arm</category>
        
        <category>YOLOv8</category>
        
      </item>
    
      <item>
        <title>Automated Grasping: Pincher X100 4-DOF Robot Arm Grasps a Purple Pen</title>
        <description>&lt;p&gt;Programmed a Pincher X100 4-DOF robot arm to grasp a purple colored pen.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/SmPIuWhf_UQ&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;I utilized an Intel Realsense D435i camera to locate a purple Northwestern pen and manipulate it using a Trossen PincherX 100 robot arm. My approach involved an initial transformation of the Realsense RGB image into the HSV color space, enabling the extraction of all purple-hued pixels using their specific HSV values. This selection formed a binary map, designating the identified pixels as white while rendering all other pixels black.&lt;/p&gt;

&lt;p&gt;Leveraging OpenCV‚Äôs contour detection capabilities, I determined the exact pixel coordinates representing the pen‚Äôs centroid. This information was pivotal in establishing the pen‚Äôs spatial orientation concerning the camera. By aligning this data with the depth image provided by the Realsense, I precisely mapped the pen‚Äôs position in the camera‚Äôs frame.&lt;/p&gt;

&lt;p&gt;Subsequently, through a conversion process, I translated the pen‚Äôs position from the camera‚Äôs frame to the robot‚Äôs frame. This enabled the seamless coordination of the robot arm‚Äôs movements towards the pen, allowing it to navigate and securely grasp the targeted object.&lt;/p&gt;

&lt;p&gt;Centroid and thresholding of the purple pen&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roy2909/roy2909.github.io/08b08e279ba67fb56615d4048d3c89ace9d4a978/assets/images/penC.gif&quot; alt=&quot;car_setup&quot; width=&quot;800&quot; /&gt;&lt;/div&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;h4&gt; &lt;a href=&quot;https://github.com/roy2909/pen_challenge&quot;&gt;View Project on Github&lt;/a&gt;&lt;/h4&gt;&lt;/div&gt;
</description>
        <pubDate>Fri, 15 Sep 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/Pen/</link>
        <guid isPermaLink="true">http://localhost:4000/Pen/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>PincherX 100</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
  </channel>
</rss>
