<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rahul Roy&apos;s Portfolio</title>
    <description>Northwestern MS in Robotics</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 12 Dec 2023 12:52:46 -0600</pubDate>
    <lastBuildDate>Tue, 12 Dec 2023 12:52:46 -0600</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Using Machine Learning to Identify Individual Ants</title>
        <description>&lt;p&gt;Machine Learning, Python, Computer Vision, OpenCV, PyTorch, C++&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;700&quot; height=&quot;394&quot; src=&quot;https://www.youtube.com/embed/KltiiPcd1nc?si=WuJL2qU8SzZCvGjD&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The goal of this project was to create a system that could identify individual ants in a colony. Using
a machine vision camera equipped with a macro lens, I was able to collect 27,244 images
of 55 different harvester ants. These images were then used to train a machine learning
network to create a feature vector, or embedding, to describe each ant. By comparing embeddings of two
ants, a threshold could be used to determine if the ants are the same or different with and accuracy 
of 86.04%.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/antID&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h3 id=&quot;why-ants&quot;&gt;Why Ants?&lt;/h3&gt;
&lt;p&gt;The first reaction when I tell people about this project is typically something along the lines of “how
is this useful?” or “why would you want to do that?”. The answer is that for most people, it’s not
useful at all. However, for myrmecologists (ant scientists), it is very useful. Ants are very social 
insects that are commonly used to study social behavior and can be used to learn more about how swarms
of robots can be used to accomplish tasks. The current way of tracking individual ants in a colony is
an extremely tedious process that involves manually labeling each ant in a colony by painting different
patterns on them. Check out &lt;a href=&quot;https://youtu.be/uAQ5IKVpysc&quot;&gt;this video&lt;/a&gt; to see exactly how tedious
this process can be. By creating a system that can automatically identify individual ants, we can save researchers a lot of time and effort.&lt;/p&gt;

&lt;h2 id=&quot;project-setup-and-hardware&quot;&gt;Project Setup and Hardware&lt;/h2&gt;
&lt;p&gt;To collect images of the ants, I used a Blackfly S camera from FLIR equipped with a macro lens. The 
camera was mounted on a microscope stand and placed above a 3D printed channel that the ants could
walk through. By using this setup, the ants were forced to walk in a straight line in the camera’s
field of view. This entire setup was then placed inside of a light box to ensure consistent lighting.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/a5261545-e3ca-49aa-b72d-d54e64cbcd73&quot; height=&quot;500&quot; width=&quot;350&quot; /&gt;  &lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/76fbd53c-77db-4546-b986-dc568ce16fc8&quot; height=&quot;500&quot; width=&quot;350&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image below shows the entire setup. There were two main living chambers for the ants. One for ants
that data had already been collected for and one for ants that data still needed to be imaged. There are
also multiple 3D printed gates to control the flow of ants from the living chambers to the data collection
area.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/cb99713f-b1f7-4a37-87c9-7e023232fd85&quot; height=&quot;800&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Setup Overview&lt;/p&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data Collection&lt;/h2&gt;

&lt;p&gt;By using the setup described above, I was able to I was able to collect 27,244 images
of 55 different harvester ants. The traffic control gates were used to isolated one ant in the data collection area. This allowed me to collect a videos of one ant at a time.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/d097f72f-aad7-4b84-8c3f-167528621f12&quot; height=&quot;250&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/9589a8a1-cd86-46d3-8b16-473048150340&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Ant Imaging Channel and Image&lt;/p&gt;

&lt;p&gt;To interact with the Blackfly camera and collect videos of the ants walking through the imaging channel, I 
had to first create a &lt;a href=&quot;https://github.com/oubrejames/antID/tree/main/black_fly_tools&quot;&gt;library in C++&lt;/a&gt;
to interface with the camera. It allowed me to control all of the camera’s settings and collect videos. I also created 
a scripts to semi-automatically collect videos of the ants.&lt;/p&gt;

&lt;p&gt;When collecting videos, I would first let an ant walk into the data collection area and then start recording. Once the ant walked under the camera enough, I would stop recording and have the ant walk to the separate living chamber. I would then repeat this process for each unfilmed ant. Videos were named sequentially as “ant_#.avi” so that I could keep track of which ant was in each video.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/c10ccb0f-a427-44be-b271-7a9c534163ca&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Pipeline&lt;/p&gt;

&lt;p&gt;Once videos of the ants were collected, I used YOLO V8 to find video frames where ants were present.
I then saved a cropped image of the ant from the YOLO detection in a folder corresponding to that
ant’s identity.&lt;/p&gt;

&lt;p&gt;Ant images are then put into the same convolutional neural network, known as a siamese 
network, that outputs a feature vector, or embedding, for each image. By finding the difference between embeddings of two images and taking the norm of that difference a distance is obtained. By comparing this
distance to a threshold, I was able to determine if ants were the same or different.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/41e57b7c-5b99-40f1-9938-27a52a565ca7&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Model Summary&lt;/p&gt;

&lt;p&gt;The model itself outputs a 128 element feature vector. To train it I used a loss function known as triplet loss. Training with triplet loss can be understood intuitively by giving the trainer 3 images at each training step, an anchor, a positive, and a negative. The anchor is an image of any ant, the positive is then a different picture of the same ant, and the negative is a picture of a different ant. During training, the model weights will be updated so that the distance between anchor and positive embeddings is smaller than the distance between anchor and negative embeddings.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/9692bb97-9344-4ca4-9df5-629caa8e4fd8&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Triplet Loss Function&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/oubrejames/antID/assets/46512429/5f83b33c-1a04-4203-a70d-9ffd9ba439ca&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Cost Function with Triplet Loss&lt;/p&gt;

&lt;p&gt;Triplet loss makes this work because the distances between the anchor positive pair minus the anchor 
negative pair plus some margin must be less than zero. Because the difference between distances must 
be negative, the anchor negative distance must be larger than the anchor positive distance. The 
margin is then used to try and push the distances from the anchor to positive and negative embeddings 
further from each other. Because the max is taken between the difference of distances and zero, the 
loss will be zero when distances are separated by the defined margin.&lt;/p&gt;

&lt;p&gt;The model was trained on 18,845 images of 45 different ants.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;To choose a threshold, I tested a range of thresholds between the average anchor-positive distance and
average anchor-negative distance. The threshold was chosen by picking the value that achieved the
highest accuracy.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot; https://github.com/oubrejames/antID/assets/46512429/973d7aff-4243-4023-a82a-7fe054b782b4&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Threshold Test&lt;/p&gt;

&lt;p&gt;The model was tested on two different test sets. One contained 2,357 different images of 45 ants that the model had seen in training and the other had 3,687 images of 10 unseen ants. On the dataset of previously
seen ants, the model achieved a true positive rate of 95.21%, a true negative rate of 99.6% and an accuracy
of 97.3%. On the dataset of unseen ants, the model achieved a true positive rate of 80.67%, a true negative rate of 93.68% and an accuracy of 86.04%. Accuracy here is defined as the number of true positives plus the number of true negative over the total number of predictions.&lt;/p&gt;

</description>
        <pubDate>Thu, 10 Aug 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/antID/</link>
        <guid isPermaLink="true">http://localhost:4000/antID/</guid>
        
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>PyTorch</category>
        
        <category>C++</category>
        
      </item>
    
      <item>
        <title>Custom built ROV/AUV</title>
        <description>&lt;p&gt;ROS2, Electrical Design, Mechanical Design, Python, Mavlink&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;700&quot; height=&quot;394&quot; src=&quot;https://www.youtube.com/embed/8WXNRmXKI5E&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://jamess-organization-15.gitbook.io/halo_auv_docs/&quot;&gt;View the Halo Documentation&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;p&gt;Halo AUV is a robot I created to serve as platform for student’s in my master’s program at Northwestern University
to use to learn about underwater robotics and expand upon. Personally, I wanted to gain experience
in underwater robotics while also building a robot from the ground up. The robot can run as a remotely
operated vehicle (ROV) and has been integrated with ROS2 to further explore autonomous capabilities. Below is a block diagram
describing the architecture of the robot.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225735537-3376c1d1-477b-4449-94be-46e569b8983c.png&quot; height=&quot;900&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Halo AUV Block Diagram&lt;/p&gt;

&lt;p&gt;On Halo itself, the Pixhawk handles low level control and sends the commands to move the motors. It 
relays depth information from a barometer and heading information from its internal sensors to the Raspberry Pi.
The Raspberry Pi sends these sensor measurements alongside the camera stream via Ethernet to a laptop.
The measurements and camera stream are then sent to either QGroundControl (ROV mode) or to ROS2 nodes
that publish the images to a ROS topic and perform different control functions. In ROV mode, control input
comes from a PlayStation controller manned by a human. When high level control commands are received, they are
sent back to the Raspberry Pi over the Ethernet tether and eventually to the Pixhawk to move the robot.&lt;/p&gt;

&lt;h2 id=&quot;mechanical-design&quot;&gt;Mechanical Design&lt;/h2&gt;
&lt;p&gt;The key aspects of the mechanical design are that it is easy to manufacture, highly adjustable, and has the ability
to easily add on new payloads/sensors. The design is also centered around the BlueRobotics 3” and 4” watertight
enclosures.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://cad.onshape.com/documents/80b1bd153eeafd48236580f8/w/a8bef987ce7a6f7e2556b87e/e/c5365bc12b4a5414eba7a777?renderMode=0&amp;amp;uiState=64133e72baa9af4c9b11d3a9&quot;&gt;View the Mechanical Design&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225687785-9b64f344-fcf0-4a4d-b7c9-24ed178947cc.png&quot; height=&quot;600&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Halo AUV&lt;/p&gt;

&lt;p&gt;The frame itself is constructed of two parallel laser-cut pieces of acrylic that are mounted to the 
water tight enclosure and reinforced with multiple standoffs. Once the top and bottom frames are cut, 
the only tools needed to assemble the frame are common hex keys. The assembly process takes about 10 minutes.&lt;/p&gt;

&lt;p&gt;The vehicle also consists of six slotted mounts along the bottom of the frame for adding and moving 
weights. These are used to adjust the vehicle’s overall weight and to align the it center of buoyancy 
with its center of gravity. This ensures that the vehicle is neutrally bouyant and level in the water.
The current design allows for weights to be added and shifted so
that adjustments can be made quickly. This is especially useful for adding new payloads to the vehicle
as they will change the weight and buoyancy balance of the robot. Additionally, floats may be added
to increase the buoyancy.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225508507-ff27e27a-fcbe-4740-a20d-61eb99ce611a.gif&quot; height=&quot;200&quot; width=&quot;280&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225509749-6b0fd71c-9194-4dd6-bf3e-4cb84fb6357f.gif&quot; height=&quot;200&quot; width=&quot;280&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225510365-459077d5-c2ea-4ec6-85bb-c2b0b1e0c81b.gif&quot; height=&quot;200&quot; width=&quot;280&quot; /&gt; &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Weight Sliders, Corner Weights, and Buoyancy Floats&lt;/p&gt;

&lt;p&gt;Additionally, payloads and sensors can also be easily added to Halo. There are many mounting holes lining
all sides of the frame and going down the center. The weight mounts are a good example of how new payloads
may be added.&lt;/p&gt;

&lt;h2 id=&quot;electrical-design&quot;&gt;Electrical Design&lt;/h2&gt;

&lt;p&gt;The following image displays Halo’s electrical layout. Everything is powered from a 14.8 V 9000 mAh
LiPo battery. The motors receive 14.8 V power through motor controllers that receive commands from the Pixhawk. 
All other components are powered off of the output of a 5 V step down converter. Communication is established from the
topside computer to the robot via Cat5e cable. The camera and Pixhawk connect to the Raspberry Pi via USB. The depth sensor
communicates with the Pixhawk through I2C. Motor positions for the thrusters and camera gimbal are communicated through PWM
from the Pixhawk to the respective motors.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225513025-a6f9a9ca-644e-4e17-b3fe-7a8860e6f7c3.png&quot; height=&quot;400&quot; width=&quot;700&quot; /&gt; &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Wiring Diagram&lt;/p&gt;

&lt;h2 id=&quot;software&quot;&gt;Software&lt;/h2&gt;

&lt;p&gt;The onboard Raspberry Pi interfaces with the Pixhawk using the Ardusub companion software. The Pixhawk is also
 running the Ardusub firmware. The robot can be used as an ROV by downloading the QGroundControl (QGC) software.
When using the robot as an ROV, a user simply has to plug the robot’s tether into their laptop and 
run QGC. This software allows someone to control the robot with a standard video game controller while getting depth information, heading information, and a first person view from onboard the robot.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/225519957-af670ed4-fafb-47be-807c-44130d4830f3.png&quot; height=&quot;400&quot; width=&quot;700&quot; /&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;QGroundControl View in ROV Mode&lt;/p&gt;

&lt;p&gt;Halo is also integrated with ROS2 through two nodes that handle control of the robot and publishing the stream from the robot’s
camera to a ROS topic.&lt;/p&gt;

&lt;p&gt;The control node initializes contact with the robot via Mavlink to send commands and receive sensor data. When 
the node first starts, it reads in the robot’s current heading and depth to hold the AUV at those values. There are then two services
to move the robot’s heading and depth either to an absolute value or by a relative amount. The controllers for heading and depth were
implemented using PI control. Similarly, there is a controller to move the robot forward, however, there is no internal sensor to measure
its forward displacement. This controller could be implemented using an external measurement like an AprilTag.&lt;/p&gt;

&lt;p&gt;The camera node reads in the camera stream from the robot via UDP. It then converts it to a gstream video feed to an OpenCV stream and then a
ROS2 sensor_msgs image message. This is then published to a ROS topic to be used by other nodes.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/halo_auv&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 15 Mar 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/halo_auv/</link>
        <guid isPermaLink="true">http://localhost:4000/halo_auv/</guid>
        
        
        <category>ROS2</category>
        
        <category>Electrical Design</category>
        
        <category>Mechanical Design</category>
        
        <category>Python</category>
        
        <category>Mavlink</category>
        
      </item>
    
      <item>
        <title>Motion Mirroring Robotic Arm</title>
        <description>&lt;p&gt;C, Mechatronics, Embedded Systems, I2C&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of Northwestern’s microprocessor design course, my group and I created an Electromyography
(EMG) controlled robotic manipulator. This allows a user to connect an EMG sensor to their arm to
open and close a robotic gripper by simply opening and closing their hand. We also combined project with 
another group in the class who were controlling a 2 DOF robotic arm’s motion based off of IMU data.
By combining these projects, we were able to create a system that would move a robot arm and control 
the gripper based off of the user’s motion.&lt;/p&gt;

&lt;h2 id=&quot;emg-gripper&quot;&gt;EMG Gripper&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6Y9bxQ33sTY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;This project was developed around the Microbit V2. Code is typically written to the Microbit in Python,
however, we were writing directly to the Microbit’s microcontroller (nRF52833) in C. The end-effector
itself is a mechanical gripper that opens and closes based off of the position of a connected servo.
We controlled this servo with a PCA9685 servo driver over I2C. Additionally, we used two sensors
in this project: an EMG sensor and a force sensitive resistor (FSR). The EMG sensor measures electrical 
signals in muscles that are generated from movement. This is attached to the users arm with electrodes
and used to detect if the hand is closed or not. The FSR measures the amount of
force that is applied to it. This is placed on the inside of the gripper and used to detect if the 
gripper is actively gripping something.&lt;/p&gt;

&lt;p&gt;The basic operation of this system is that the output of the two sensors are constantly being read
by the Microbit. If the EMG sensor gives a high signal, then the user is trying to grasp. When this
is detected, PWM signals are sent to the servo driver over I2C in a loop, closing the gripper incrementally
with each iteration. When an item is grasped, a forced is applied to the FSR, producing a 
high signal that is used to break the grasping loop and hold the gripper at a constant position.
Once, the user releases their grasp, the gripper will then start to incrementally open, unless
interrupted by another grasp command.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/emgripper&quot;&gt;View EMG Gripper Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;emg-gripper--imu-arm&quot;&gt;EMG Gripper + IMU Arm&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xjytBSXibu4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;When the EMG gripper is combined with the IMU controlled arm, the resulting system allows a user to
manipulate the robot to perform various actions just by moving their own arm and hand.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;h4 id=&quot;emg-gripper-1&quot;&gt;EMG Gripper&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;David Dorf&lt;/li&gt;
  &lt;li&gt;Katie Hughes&lt;/li&gt;
  &lt;li&gt;James Oubre&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;imu-arm&quot;&gt;IMU Arm&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nicolas Morales&lt;/li&gt;
  &lt;li&gt;Hang Yin&lt;/li&gt;
  &lt;li&gt;Felipe Jannarone&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 11 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Gripper/</link>
        <guid isPermaLink="true">http://localhost:4000/Gripper/</guid>
        
        
        <category>C</category>
        
        <category>Mechatronics</category>
        
        <category>Embedded Systems</category>
        
        <category>I2C</category>
        
      </item>
    
      <item>
        <title>BotChocolate</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, ROS2, Python, Motion Planning, Moveit, Intel Realsense, Emika Franka Robot Arm&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/Q_aNWWe4h5M&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The purpose of this project was to create hot chocolate using the Franka Emika 7 DOF robot arm. To perceive its environment, the system utilizes an Intel D435i camera AprilTags. Upon initial setup, a calibration process must be completed. After this, the process consisted of using AprilTags to locate a scoop of cocoa, a mug, a spoon, and a kettle relative to the robot. Next, using our custom MoveIt API for Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;movebot&lt;/code&gt;, the robot is able to perform path planning between all of these objects. It turns on the kettle, dumps the cocoa powder into the mug, pours the hot water over the power, and stirs the mixture it with the spoon.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/bot_chocolate&quot;&gt;View BotChocolate Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;moveit-api&quot;&gt;Moveit API&lt;/h2&gt;
&lt;p&gt;Because the ROS2 MoveIt package does not have a Python API yet, creating one was the first step. By 
looking into the MoveIt ROS2 actions and services, we were able to create an API that takes either 
Cartesian coordinates of the Franka end-efffector or joint-state positions of each joint and creates
a path to the specified position. It also allows for the choice of moving in a Cartesian path.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Sed9XwHT-7c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;
&lt;p&gt;The vision system is comprised of two major parts: calibration and component location. AprilTags are
used to detect where each of the hot chocolate making components are. The AprilTags are located using
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltag_ros&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;There is one tag fixed to a kettle and one tag fixed to a jig that has slots for a mug, cocoa scooper,
and a spoon. Once these tags are found in the camera frame, there are two transformation trees 
describing where all the hot chocolate components are relative to the camera and where each link of
the Franka robot arm is relative to the robot’s base. However, we need to connect these trees to 
get the location of each component relative to the robot’s base. This is where the calibration is 
used. An AprilTag must be placed in the robot’s gripper, aligned with the gripper’s
coordinate frame and in view of the camera to calibrate. We then wrote a program that finds the location of the tag
and relates it to the position of the base, connecting the robot frame to the camera frame. We then
save this transformation to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.yaml&lt;/code&gt; file to be used later on when running the main hot chocolate
making sequence.&lt;/p&gt;

&lt;p&gt;After obtaining the transformation from the camera to the robot, the robot can now use the locations
of the two AprilTags to locate and manipulate the hot chocolate components as necessary.&lt;/p&gt;

&lt;h2 id=&quot;motion-planning&quot;&gt;Motion Planning&lt;/h2&gt;
&lt;p&gt;Once the locations of all the components are known, it’s time to make hot chocolate. Cartesian paths
are used to move the robot in straight lines and rotational path planning is used for achieving motions
like tilting to grab the scoop and dumping the cocoa. Both Cartesian and rotational motion are used to achieve pouring.
To make a cup of hot chocolate, the robot first turns on the kettle to heat the water, grab the cocoa scoop, 
pours it in a mug, pours the hot water over the powder, and stirs. We also used serval intermediate 
waypoints to help avoid reaching the joint limits of the robot.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Shantao Cao&lt;/li&gt;
  &lt;li&gt;Allan Garcia-Casal&lt;/li&gt;
  &lt;li&gt;Nicholas Marks&lt;/li&gt;
  &lt;li&gt;James Oubre&lt;/li&gt;
  &lt;li&gt;David Dorf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/206768445-4503edc2-2075-48b4-baf7-e6dc7bd3ca86.png&quot; alt=&quot;bot_choc-min&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Botchoc/</link>
        <guid isPermaLink="true">http://localhost:4000/Botchoc/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>ROS2</category>
        
        <category>Python</category>
        
        <category>Motion Planning</category>
        
        <category>Moveit</category>
        
        <category>Intel Realsense</category>
        
        <category>Emika Franka Robot Arm</category>
        
      </item>
    
      <item>
        <title>Physics Simulation</title>
        <description>&lt;p&gt;Python, Dynamic Systems, Simulation, Jupyter Notebook&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/oubrejames/oubrejames.github.io/4b9a9bb1b282e894d9406c93f0adbf26e34a60b5/assets/images/cropped_jack.gif&quot; alt=&quot;jack_box_gif&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Using Lagrangian dynamics, I was able to simulate a box bouncing around within a larger box. First,
I calculated the potential and kinetic energies of the system to obtain its Lagrangian and later on,
the Euler-Lagrange equations. These equations describe the dynamics of the system while not
experiencing impact. To detect impact the following coordinate frames were used.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208279053-1a1dd404-148d-4dbd-aadc-0b7c9d14c366.png&quot; alt=&quot;jack_box_gif&quot; width=&quot;500&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Using the transformation matrices between these frames, I am able to detect impact if any of the 
smaller box’s vertices intersect the larger box’s walls. Each vertex is checked against each wall 
for every time step of the simulation. If an impact is detected, the dynamic equations are updated and
the simulation continues.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/physics-simulator&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;
</description>
        <pubDate>Sun, 04 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/physics_simulator/</link>
        <guid isPermaLink="true">http://localhost:4000/physics_simulator/</guid>
        
        
        <category>Python</category>
        
        <category>Dynamic Systems</category>
        
        <category>Simulation</category>
        
        <category>Jupyter Notebook</category>
        
      </item>
    
      <item>
        <title>Mobile Manipulator Simulation</title>
        <description>&lt;p&gt;Mobile Manipulation / CoppeliaSim / PID Control / Python&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QY0E-IW8qvQ&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As the capstone project for Northwestern’s Robotic Manipulation course I simulated a KUKA youBot
moving to a cube, grasping it, and then placing it in a desired position. First, I created a function
to generate trajectories from the robot’s starting position to all the different waypoints to 
complete the task. The homogeneous transformation matrices between waypoints are used to create a 
screw or Cartesian trajectories between points. These trajectories are then stored in a matrix representing 
the desired movement of the robot’s chassis and end effector. Next, I implemented a PID feedback
controller based off of the robot’s current position and odometry to ensure that the robot can self
correct if error is introduced. Lastly, the actual positions of the robot are stored in a CSV file
and used to simulate the robot in Coppeliasim.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The simulation completed the task in three different scenarios: with poor PID gains, tuned
PID gains, and a different start and end configuration for the cube.  Overall, each 
simulation is able to reduce to zero error before the robot gets the the cube’s standoff position.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/youbot_simulation&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 01 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/robman/</link>
        <guid isPermaLink="true">http://localhost:4000/robman/</guid>
        
        
        <category>Mobile Manipulation / CoppeliaSim / PID Control / Python</category>
        
      </item>
    
      <item>
        <title>Pen Stealing Robot</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, Python, PincherX 100, Intel Realsense&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/JoxrQ2MmBp4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;As part of Northwestern’s orientation hackathon, I used an Intel Realsense D435i camera to detect
the location of a purple Northwestern pen and grab it with a Trossen PincherX 100 robot arm. 
To detect the pen, I converted the RGB image from the Realsense to an HSV image and used the HSV values
to find all purple pixels. Next, I created a binary map where the detected pixels were shown as 
white and all other pixels black. Using this and OpenCv’s contour detection I found the pixel location
of the centroid of pen. This allowed me to find the location of the pen relative to the camera using 
the aligned depth image generated from the Realsense. I then convert the pen position from the camera’s
frame to the robot’s frame and control the robot to move to the pen and grasp it.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/pen_challenge&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 15 Sep 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/Pen/</link>
        <guid isPermaLink="true">http://localhost:4000/Pen/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>PincherX 100</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
      <item>
        <title>Autonomous Sanding</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, UR5 Robot Arm, Manufacturing, Research, Intel Realsense&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;1080&quot; height=&quot;620&quot; src=&quot;https://www.youtube.com/embed/gyQQtJ2jm6k&quot; title=&quot;robotic_sanding&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;While getting my bachelor’s degree at Louisiana State University I worked as an undergraduate researcher
in the Innovation in Control and Robotics Engineering Lab (iCORE Lab). My main area of research was
computer vision for robotic systems. Specifically, as part of an NSF funded research project, I
created a computer vision system to detect surface defects in fiber glass for autonomous sanding.
Once, the locations of the defects were found, two types of path planning were used to create a sanding
path. The path waypoints were then relayed to a UR5e 6 DOF robot arm with a sander attachment to 
sand the defected areas.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208994781-0e1e5a9e-4540-45f4-ad3e-13374b4bca35.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Pipeline&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208988986-65a70827-1d47-4bcf-aab6-9a6e2c07246c.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Components&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/physics-simulator&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt; --&gt;
&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S240589632201031X/pdf?md5=6790ab4229b08c78bb91bb6a34d5e885&amp;amp;pid=1-s2.0-S240589632201031X-main.pdf&quot;&gt;Read the paper&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;
&lt;p&gt;Using traditional computer vision techniques like canny edge detection, morphological closing, 
contour detection, and binary mapping I was able to detect and isolate defects present on the surface
of a fiber glass panel.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208990411-230117af-4a57-4276-bc6e-0b72b197a123.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Computer Vision Pipeline&lt;/p&gt;

&lt;p&gt;First, an Intel D435i camera captures the RGB and depth images of the fiber glass sample. A canny 
edge detector is applied to the RGB image to separate the inherit pattern visible underneath the 
surface of the fiber glass from the defects present on the surface. After this step, the surface 
defects become easily visible. To create a general region of the defective area, I use morphological
closing to merge nearby edges and fill openings within a certain area to create a blob like structure
where defects reside. Next I use contour detection to get the pixel locations of everything within
the defective regions.&lt;/p&gt;

&lt;h2 id=&quot;path-planning&quot;&gt;Path Planning&lt;/h2&gt;
&lt;p&gt;Once the locations of the defects are known, a path must be created so the robot can sand over all
of the defective areas. Two different types of path planning were implemented. Multi-goal path planning
was needed to make sure the robot goes to each of the separate defective regions and coverage path
planning was used to ensure the robot sanded all of the defects within a given region.&lt;/p&gt;

&lt;p&gt;For multi-goal path planning, a nearest neighbor algorithm was implemented to create a path that 
went to each region based off of the location of the regions centroid. Next, a grid-based sweeping 
algorithm is used to create a path that covers the entire area. Finally, the two plans are combined
and the robot’s sander is pressed against the piece for the coverage paths to ensure sanding and it 
is offset from the piece during multi-goal movements so that smooth sections are not sanded erroneously.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208995195-854b8fea-4304-4bf5-8cb5-7975b003edc2.png&quot; height=&quot;425&quot; width=&quot;425&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208995475-84289c70-9f8f-4b67-a78f-b1ce5674cb63.png&quot; height=&quot;425&quot; width=&quot;425&quot; /&gt; &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Example of Finalized Path in 2D and 3D&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The system successfully detected the defected areas and we quantified the results by having a person
manually label defects in one of the fiber glass image samples and compared that to what the system
detected. Fifteen fiber glass panels were used and the calculated average sensitivity obtained was 
66.24%, the average specificity was 78.20%, and the resulting accuracy was 81.02%. Furthermore, 
using a profilometer, I measured the surface roughness of certain defected areas before and after 
sanding and found that the average roughness at these areas was about half as rough after performing
autonomous sanding.&lt;/p&gt;

&lt;h2 id=&quot;contributors&quot;&gt;Contributors&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;William Ard&lt;/li&gt;
  &lt;li&gt;Corina Barbalata&lt;/li&gt;
  &lt;li&gt;Joshua Nguyen&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 22 May 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/icore/</link>
        <guid isPermaLink="true">http://localhost:4000/icore/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>UR5 Robot Arm</category>
        
        <category>Manufacturing</category>
        
        <category>Research</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
      <item>
        <title>Cocktail Maker</title>
        <description>&lt;p&gt;Arduino, Circuit Design, Sensors &amp;amp; Actuators&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;424&quot; height=&quot;754&quot; src=&quot;https://www.youtube.com/embed/AqyQ7yTlyfM&quot; title=&quot;cocktail maker&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As the final project for my Sensors and Actuators class at LSU, I designed an automated cocktail maker.
Using three ultrasonic distance sensors offset at different heights, the system can detect three different 
heights of cups corresponding to a wine glass, pint glass, and a cocktail glass. I implemented logic
on an Arduino to decide what kind of glass is present based off the sensor outputs. Once the type of
cup is known, the Arduino turns on pumps connected to the corresponding cocktail ingredients. Originally,
the system was supposed to dispense wine for the wine glass, beer for the pint, and a mixed drink for
the cocktail glass. However, beer did not dispense properly from the pump and it mostly dispensed
foam so I changed it to dispense a different mixed drink. I also did not actually have any wine when 
making the video so I used water. 
The logic used is shown in the pipeline below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209448153-3b40e4dd-774a-401b-9d44-19efafc30134.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Pipeline&lt;/p&gt;

&lt;h2 id=&quot;electrical-design&quot;&gt;Electrical Design&lt;/h2&gt;
&lt;p&gt;The electrical design is shown in the image below. The system is powered from a wall outlet to a 12
V power supply. A 7 V voltage regulator is used to power the Arduino and the 12 volts powers a relay
 module which, in turn, powers the pumps. To actually dispense the liquid, the Arduino sends a high
 signal to the corresponding pin on the relay module to close the relay and turn on the pump.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209448159-8fa2b899-9e08-43c5-86b6-f411c93f6046.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Electrical Schematic&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/pen_challenge&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt; --&gt;
</description>
        <pubDate>Wed, 13 Apr 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/cocktail/</link>
        <guid isPermaLink="true">http://localhost:4000/cocktail/</guid>
        
        
        <category>Arduino</category>
        
        <category>Circuit Design</category>
        
        <category>Sensors &amp; Actuators</category>
        
      </item>
    
      <item>
        <title>Electric Formula Racecar</title>
        <description>&lt;p&gt;Circuit Design, Power Systems, PCB Design, Digital Logic, Mechanical Design, Manufacturng&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/oubrejames/oubrejames.github.io/gh-pages/assets/images/wide_fsae1.gif&quot; height=&quot;400&quot; width=&quot;1400&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;For my senior capstone project, I was a part of a group that converted an internal combustion FSAE
racecar to be fully electric. The electrical side of the project consisted of creating a custom high
voltage battery pack (accumulator) with a battery management system to power the vehicle’s motor. There was also 
a low voltage subsystem composed of a device to check for braking and accelerating faults, a circuit
to cut off high voltage power in dangerous scenarios, and 12 V bus for miscellaneous electronics. Furthermore,
because we were converting an existing internal combustion vehicle, certain mechanical alterations
were made. Personally, I was responsible for the low voltage subsystem and acted as team treasurer.&lt;/p&gt;

&lt;h2 id=&quot;low-voltage-electrical&quot;&gt;Low Voltage Electrical&lt;/h2&gt;
&lt;p&gt;The low voltage system is broken up into three main parts: the acceleration and 
brake fault detection device, the safety shutdown circuit, and the 12 V bus. The purpose of this 
subsystem is to handle all auxiliary functions of the vehicle. Basically any function of the car 
outside of the high voltage power train is a part of the low voltage system. Below is the circuit 
diagram for the vehicles auxiliary systems.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211061123-7a96fe45-fb80-4c00-98a7-ed8b1cfdd7d9.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Low Voltage Auxillory System Circuit Diagram&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211060715-f450c295-2451-442d-9807-a751f7e2a5b5.png&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; Auxillory System PCBs&lt;/p&gt;

&lt;h3 id=&quot;acceleration-and-brake-fault-detection-device&quot;&gt;Acceleration and Brake Fault Detection Device&lt;/h3&gt;
&lt;p&gt;One key feature of this project is the acceleration and brake fault detection device. This device 
detects the following accelerating and braking conditions that could cause harm to the driver:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simultaneous pressing of accelerator and brake pedals&lt;/li&gt;
  &lt;li&gt;Significant current delivered to motor while driver is hard braking&lt;/li&gt;
  &lt;li&gt;Short or open circuits in the accelerator and break pedal sensors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If any of these faults were detected, a trigger signal is sent to a 500 ms timer which would then 
actuate a relay to open the safety shutdown circuit and cut off high voltage power. The logic is 
described in the diagram below.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211351660-95b478a3-97a4-4c37-b137-3bf4f9780392.png&quot; height=&quot;500&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Accelerator and Brake Pedal Fault Detection Logic&lt;/p&gt;

&lt;p&gt;Per FSAE rules, the device had to be compeltely analog. Because of this, I chose to detect faults 
using op-amp comparators and logic gates. If the input signal from the sensor went above a certain 
value or outside a specified range, the comparator would send a high signal to 500 ms timer. Below is
the circuit diagram for the device.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211362295-e465339e-2c05-4d23-8f41-d82d6a0bb8dc.png
&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Accelerator and Brake Pedal Fault Detection Circuit Diagram&lt;/p&gt;

&lt;p&gt;Using EaglePCB, I created the manufacturing drawings for the board’s PCB. I then ordered the board 
and soldered the components on myself.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211633971-c9f89743-3217-4d98-9eeb-a563323437c3.png&quot; height=&quot;300&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Accelerator and Brake Pedal Fault Detection PCB&lt;/p&gt;

&lt;h3 id=&quot;safety-shutdown-circuit&quot;&gt;Safety Shutdown Circuit&lt;/h3&gt;

&lt;p&gt;A safety shutdown circuit was created to cut off high powered electricity in unsafe conditions. This
circuit connects the 12 V battery to the coils of the two high power relays that are connected to 
the vehicle’s high power battery or accumulator. The rest of the circuit is a number of switches and
relays in series such that if any are disconnected, the circuit is open and no voltage is applied to 
the accumulator relay coils. When there is no voltage at these coils, they are open and the accumulator
is disconnected.&lt;/p&gt;

&lt;p&gt;The circuit has 3 emergency stop buttons, 2 master switches, a brake over travel 
switch, and 2 low voltage relays. The brake over travel switch is a toggle switch located behind the 
break pedal such that if the brakes are pressed extremely hard, the flip is switched and the circuit 
is opened. The 2 low voltage relays are connected to the acceleration and brake fault detection device 
(shown as APPS/BSPD) and the insulation monitoring device (IMD). The IMD measures the insulation 
between the high voltage bus and the chassis ground. If a short between the two is detected, the 
device outputs a high signal to open the relay in the shutdown circuit.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211060489-fb3fc2e4-7bef-4339-ba6c-8e091cb9c5dd.png&quot; height=&quot;600&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Safety Shutdown Circuit&lt;/p&gt;

&lt;h3 id=&quot;low-voltage-power&quot;&gt;Low Voltage Power&lt;/h3&gt;
&lt;p&gt;Part of this project was ensuring it could withstand the FSAE endurance event which we calculated would
take about 30 minutes. To spec out the battery for this, I found the max current draw of each component
on the 12 V bus to be 34.6 A. To complete the endurance event the vehicle required a battery with a
capacity of 17.3 Ah.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211062386-db1ac433-20ec-4532-a877-260ac9ebe49a.png&quot; height=&quot;400&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;12 V Bus&lt;/p&gt;

&lt;h2 id=&quot;high-voltage-electrical&quot;&gt;High Voltage Electrical&lt;/h2&gt;

&lt;p&gt;The high voltage powertrain shown below consists of the high voltage battery pack (accumulator), 
accumulator isolation relays, high voltage disconnects, motor controller, and the motor. The accumulator
isolation relays and high voltage disconnects are used to isolate the accumulator when not driving. 
The high voltage lines are then fed into the motor controller which operates the motor based on the
input from the accelerator pedal position sensors.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211638355-cd7efaf9-2df1-45a8-9109-6f4d6a332829.png&quot; height=&quot;600&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;High Voltage Bus&lt;/p&gt;

&lt;p&gt;The accumulator itself was made of 756 18650 Li-ion rechargeable cells broken up into 9 parallel 
connections of 84 series connections. Because the motor we selected was a 300 V motor, we required a
300 V battery pack. After performing a simulation of the endurance event it was determined that a 
battery required at least a 6.3 kWh capacity at this voltage to finish. Because 18650 cells have a 
nominal voltage of 3.6 V, we concluded that 84 cells in series were necessary. Furthermore, to reach 
the current requirement (21 Ah), we calculated that 9 cells in parallel were needed since the cell’s 
nominal capacity is 2.5 Ah. We also used an off-the-shelf battery management system to ensure proper
cell balancing and charging.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211061422-5a3f2ce8-8b3e-4308-9254-78ec03287e79.png&quot; height=&quot;300&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Battery Model&lt;/p&gt;

&lt;h3 id=&quot;manufacturing&quot;&gt;Manufacturing&lt;/h3&gt;
&lt;p&gt;To assemble the battery pack, series connections were made by spot welding positive and negative
terminals together with nickel strips. Parallel connections were made by screwing thicker nickel
strips into a copper bus bar.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211060775-4c267d84-7744-4ab6-8959-b27025ffc28a.png&quot; height=&quot;400&quot; width=&quot;325&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211063614-2411203c-d497-4b3b-930b-90057dc83ef7.png&quot; height=&quot;400&quot; width=&quot;325&quot; /&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Battery Segment Manufacturing&lt;/p&gt;

&lt;h2 id=&quot;mechanical&quot;&gt;Mechanical&lt;/h2&gt;
&lt;p&gt;The mechanical systems of the vehicle are shown below. Changes to the drivetrain and cooling system 
were necessary when changing from internal combustion to electric and a battery housing was needed
to protect and insulate the battery segments.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211065647-dfa76530-b426-4cb4-9a31-2b71c24d21be.png&quot; height=&quot;400&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Mechanical Systems&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Douglas Byrd&lt;/li&gt;
  &lt;li&gt;Muhammed Al-Hassani&lt;/li&gt;
  &lt;li&gt;Esther Yoo&lt;/li&gt;
  &lt;li&gt;Michael Hom&lt;/li&gt;
  &lt;li&gt;Felix Rodrigue&lt;/li&gt;
  &lt;li&gt;Jacob Antie &lt;/li&gt;
  &lt;li&gt;Alejandro Nunez&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/211373131-5975e0fe-ff09-467c-9f22-22d847f0455a.jpeg&quot; height=&quot;500&quot; width=&quot;1100&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 23 May 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/fsae/</link>
        <guid isPermaLink="true">http://localhost:4000/fsae/</guid>
        
        
        <category>Circuit Design</category>
        
        <category>Power Systems</category>
        
        <category>PCB Design</category>
        
        <category>Digital Logic</category>
        
        <category>Mechanical Design</category>
        
        <category>Manufacturng</category>
        
      </item>
    
  </channel>
</rss>
