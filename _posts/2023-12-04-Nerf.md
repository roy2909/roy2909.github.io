---
layout: post
title:  "Nerf shoots"
categories: [Computer Vision, OpenCV, ROS2, Python, Motion Planning, Moveit, Intel Realsense,Emika Franka Robot Arm]
image: assets/images/nerfy.gif
featured: true
hidden: true
---
Programmed a 7 DOF Emika Franka Arm to detect and shoot down colored bowling pins.

<div align="center"><iframe width="720" height="400" src="https://www.youtube.com/embed/Q_aNWWe4h5M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>

## Overview
In this project, the Franka arm searches for known randomly placed pins and shoots them down. It is fitted with an onboard camera to see its environment and a Nerf blaster to be able to shoot the targets. The user can specify the color of the pins that the arm is going to shoot first and then once all of those colored pins are shot, the arm will wait for a second input to continue shooting different colored targets.

<div align="center"><h2> <a href="https://github.com/oubrejames/bot_chocolate">View BotChocolate Github</a></h2></div>


## Moveit API
Because the ROS2 MoveIt package does not have a Python API yet, creating one was the first step. By 
looking into the MoveIt ROS2 actions and services, we were able to create an API that takes either 
Cartesian coordinates of the Franka end-efffector or joint-state positions of each joint and creates
a path to the specified position. It also allows for the choice of moving in a Cartesian path.

<div align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/Sed9XwHT-7c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>


## Vision
The vision system is comprised of two major parts: calibration and component location. AprilTags are
used to detect where each of the hot chocolate making components are. The AprilTags are located using
the `apriltag_ros` package. 

There is one tag fixed to a kettle and one tag fixed to a jig that has slots for a mug, cocoa scooper,
and a spoon. Once these tags are found in the camera frame, there are two transformation trees 
describing where all the hot chocolate components are relative to the camera and where each link of
the Franka robot arm is relative to the robot's base. However, we need to connect these trees to 
get the location of each component relative to the robot's base. This is where the calibration is 
used. An AprilTag must be placed in the robot's gripper, aligned with the gripper's
coordinate frame and in view of the camera to calibrate. We then wrote a program that finds the location of the tag
and relates it to the position of the base, connecting the robot frame to the camera frame. We then
save this transformation to a `.yaml` file to be used later on when running the main hot chocolate
making sequence. 

After obtaining the transformation from the camera to the robot, the robot can now use the locations
of the two AprilTags to locate and manipulate the hot chocolate components as necessary.

## Motion Planning
Once the locations of all the components are known, it's time to make hot chocolate. Cartesian paths
are used to move the robot in straight lines and rotational path planning is used for achieving motions
like tilting to grab the scoop and dumping the cocoa. Both Cartesian and rotational motion are used to achieve pouring.
To make a cup of hot chocolate, the robot first turns on the kettle to heat the water, grab the cocoa scoop, 
pours it in a mug, pours the hot water over the powder, and stirs. We also used serval intermediate 
waypoints to help avoid reaching the joint limits of the robot.

## The Team
* Shantao Cao
* Allan Garcia-Casal
* Nicholas Marks
* James Oubre
* David Dorf

![bot_choc-min](https://user-images.githubusercontent.com/46512429/206768445-4503edc2-2075-48b4-baf7-e6dc7bd3ca86.png)
