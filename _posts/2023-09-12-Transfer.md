---
layout: post
title:  "Explainable AI for Hand Gesture Recognition"
categories: [ Machine Learning, Python, Deep Learning, Explainable AI ]
image: 
featured: false
hidden: false
---
<style>
  a {
    color: blue; /* Set the color of links to purple */
  }
</style>
Using Explainable AI to classify hand gestures - Post in progress

## Overview
This project aims to improve upon a previously developed hand gesture recognition system by incorporating Explainable AI techniques. The goal is to enhance the model's interpretability and transparency, allowing users to understand how the model makes predictions and providing insights into its decision-making process. By visualizing the model's internal mechanisms and highlighting key channels, we can create a more user-friendly system for hand gesture recognition.



## Background

**Dataset**

The dataset was a publically available dataset of hand gestures, based on this [paper](https://www.sciencedirect.com/science/article/pii/S1746809420301373?via%3Dihub) and code available [here](https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset). The dataset comprises surface EMG signals recorded from 5 participants over 10 days, using the Myo Armband (Thalmic Labs), a wearable sensor with 8 dry electrodes designed for ease of use without skin preparation. Signals were captured at three sensor positions—neutral, 8-mm inward rotation, and 8-mm outward rotation—randomly across 30 days. Each participant performed 22 gestures, including single- and two-degree-of-freedom wrist and hand motions, with 4 repetitions per gesture daily. EMG signals, sampled at 200 Hz and high-pass filtered at 15 Hz, were segmented using a sliding window of 250 ms with 80% overlap, capturing 1.5-second trials representing muscle activity during gestures. The dataset structure, shaped as 4 × 22 × 26 × 8 (repetitions × gestures × segments × channels).



**Transfer Learning**

The original model based on this [paper](https://www.biorxiv.org/content/10.1101/2022.01.06.475232v1.abstract) was trained using transfer learning, a machine learning technique where a model trained on one dataset (the "source domain") is adapted to perform well on a different but related dataset (the "target domain"), often with minimal additional training. This approach is particularly effective when the target domain lacks sufficient labeled data, as it allows the model to leverage knowledge learned from the source domain to generalize better in new contexts.

The source domain consists of labeled surface EMG signals collected from specific users on particular days at a fixed sensor position (e.g., neutral position). The target domain involves unlabeled EMG signals from different users recorded on multiple days but at the same sensor position. The variability between users and recording days introduces challenges due to physiological differences and temporal variations in the EMG signals.

In myoelectric control systems, labeled data from user calibration sessions is essential but transient, as sEMG signals degrade over time, requiring frequent recalibration to maintain performance. However, during regular use, unlabeled data is continuously generated, making unsupervised domain adaptation a natural solution, with the calibration session as the labeled source and ongoing usage data as the unlabeled target.

By using SpectroConvNet, a deep learning model trained on spectrogram-based features, and adapting it with SCADANN (Self-Calibrating Asynchronous Domain Adversarial Neural Network) both described [here](https://ieeexplore.ieee.org/document/9207910/citations?tabFilter=papers#citations), the study demonstrates how transfer learning can improve gesture classification accuracy across users and days without requiring additional labeled data. While SpectroConvNet combined with DANN(Domain Adversarial Neural Network), fed into SCADANN achieves impressive performance in adapting to new users and days, it functions as a black-box model, meaning its internal workings and decision-making processes are not easily interpretable. This lack of transparency makes it challenging to understand which features or patterns in the EMG signals are driving the predictions.



**Previous Results**

Accuracy of each model and how it was improved by transfer learning and SCADANN with graphs and tables

**Shapley Values**

Shapley values are a method from cooperative game theory that assigns a value to each feature based on its contribution to the model's prediction. By calculating the Shapley values for each feature in the input data, we can determine the relative importance of different channels in the EMG signals and visualize how they influence the model's decision-making process. This provides valuable insights into which muscle groups or movements are most relevant for classifying hand gestures and helps users understand the model's behavior more intuitively.

***Paper Summary***


***Network Adaptation:***



**Results after adaptaion**


## Future Work

&nbsp;
<!-- <div align="center"><h4> <a href="https://github.com/roy2909/InteractiveSketch">View Project on Github</a></h4></div> -->





