---
layout: post
title:  "Explainable AI for Hand Gesture Recognition - Post in Progress"
categories: [ Machine Learning, Python, Deep Learning, Explainable AI ]
image: 
featured: false
hidden: false
---
<style>
  a {
    color: blue; /* Set the color of links to purple */
  }
</style>
Using Explainable AI to classify hand gestures - Post in progress

## Overview
This project aims to improve upon a previously developed hand gesture recognition system by incorporating Explainable AI techniques. The goal is to enhance the model's interpretability and transparency, allowing users to understand how the model makes predictions and providing insights into its decision-making process. By visualizing the model's internal mechanisms and highlighting key channels, we can create a more user-friendly system for hand gesture recognition.

## Background

**Dataset**

The dataset was a publically available dataset of hand gestures, based on this [paper](https://www.sciencedirect.com/science/article/pii/S1746809420301373?via%3Dihub) and code available [here](https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset). The dataset comprises surface EMG signals recorded from 5 participants over 10 days, using the Myo Armband (Thalmic Labs), a wearable sensor with 8 dry electrodes designed for ease of use without skin preparation. Signals were captured at three sensor positions—neutral, 8-mm inward rotation, and 8-mm outward rotation—randomly across 30 days. Each participant performed 22 gestures, including single- and two-degree-of-freedom wrist and hand motions, with 4 repetitions per gesture daily. EMG signals, sampled at 200 Hz and high-pass filtered at 15 Hz, were segmented using a sliding window of 250 ms with 80% overlap, capturing 1.5-second trials representing muscle activity during gestures. The dataset structure, shaped as 4 × 22 × 26 × 8 (repetitions × gestures × segments × channels). This is then processed into formatted spectrograms of shape (4, 8, 10)

**Transfer Learning**

The original model based on this [paper](https://www.biorxiv.org/content/10.1101/2022.01.06.475232v1.abstract) was trained using transfer learning, a machine learning technique where a model trained on one dataset (the "source domain") is adapted to perform well on a different but related dataset (the "target domain"), often with minimal additional training. This approach is particularly effective when the target domain lacks sufficient labeled data, as it allows the model to leverage knowledge learned from the source domain to generalize better in new contexts.

The source domain consists of labeled surface EMG signals collected from specific users on particular days at a fixed sensor position (e.g., neutral position). The target domain involves unlabeled EMG signals from different users recorded on multiple days but at the same sensor position. The variability between users and recording days introduces challenges due to physiological differences and temporal variations in the EMG signals.

In myoelectric control systems, labeled data from user calibration sessions is essential but transient, as sEMG signals degrade over time, requiring frequent recalibration to maintain performance. However, during regular use, unlabeled data is continuously generated, making unsupervised domain adaptation a natural solution, with the calibration session as the labeled source and ongoing usage data as the unlabeled target.

By using SpectroConvNet, a deep learning model trained on spectrogram-based features, and adapting it with SCADANN (Self-Calibrating Asynchronous Domain Adversarial Neural Network) both described [here](https://ieeexplore.ieee.org/document/9207910/citations?tabFilter=papers#citations), the study demonstrates how transfer learning can improve gesture classification accuracy across users and days without requiring additional labeled data. While SpectroConvNet combined with DANN(Domain Adversarial Neural Network), fed into SCADANN achieves impressive performance in adapting to new users and days, it functions as a black-box model, meaning its internal workings and decision-making processes are not easily interpretable. This lack of transparency makes it challenging to understand which features or patterns in the EMG signals are driving the predictions.

**Previous Results**

Accuracy of each model and how it was improved by transfer learning and SCADANN with graphs and tables
![Previous Results](/assets/images/previou_results.png)

**Shapley Values**

Shapley values are a method from cooperative game theory that assigns a value to each feature based on its contribution to the model's prediction. By calculating the Shapley values for each feature in the input data, we can determine the relative importance of different channels in the EMG signals and visualize how they influence the model's decision-making process. This provides valuable insights into which muscle groups or movements are most relevant for classifying hand gestures and helps users understand the model's behavior more intuitively.

**Paper Summary**

The paper I used to incorporate the Shapley value method is this [ paper](https://www.sciencedirect.com/science/article/pii/S0020025524005802#fm0050). It introduces a method leveraging Shapley values for understanding and enhancing gesture recognition from sEMG signals. By treating each electrode channel as a "player" in a cooperative game, the Shapley value (\(\phi_n\)) quantifies each channel's contribution to recognizing gestures.

***Total Contribution***

The total contribution of all channels (\(C\)) is calculated as the difference in model output with and without all input channels:  
$$
C = f(\text{all channels}) - f(\emptyset)
$$
Where:
- \(C\): Total contribution of all electrode channels in gesture recognition.
- \(f(\text{all channels})\): Model output when all channels' signals are used as input.
- \(f(\emptyset)\): Model output when no channel signal is used as input (zero input).

***Shapley Value***

The Shapley value for a channel \(n\) is computed by averaging its marginal contribution across all possible coalitions \(S \subseteq N\):  
$$
\phi_n = \sum_{S \subseteq (N \setminus \{n\})} \frac{|S|! \cdot (|N| - |S| - 1)!}{|N|!} \cdot \left[f(S \cup \{n\}) - f(S)\right]
$$
Where:
- \(\phi_n\): Shapley value of the \(n\)-th channel, indicating its contribution to gesture recognition.
- \(N\): Total set of electrode channels.
- \(S\): Subset of channels excluding channel \(n\).
- \(|S|\): Number of channels in subset \(S\).
- \(f(S \cup \{n\})\): Model output when subset \(S\) plus channel \(n\) is used as input.
- \(f(S)\): Model output when only subset \(S\) is used as input.
These values are aggregated to form a matrix across gestures, enabling a global analysis of each channel's importance.

***Enhancing Gesture Recognition Using Shapley Feedback***

Shapley values guide model improvement by computing a contribution ratio (\(w_n\)) for each channel, normalized over gestures:  
$$
w_n = \frac{\phi_n}{\sum_k \phi_k}
$$
Where:
- \( w_n \): Contribution ratio of the \( n \)-th channel, normalized Shapley value.
- \( \phi_n \): Shapley value of the \( n \)-th channel.
- \( \sum_k \phi_k \): Sum of all Shapley values for all channels.

This ratio is integrated into the first convolutional layer of the DNN, weighting the convolutional kernel to prioritize important channels:  
$$
y = w_n \cdot \text{Conv}(x)
$$
Where:
- \(y\): Weighted output after applying the convolution operation.
- \(w_n\): Contribution ratio of the \(n\)-th channel.
- \(\text{Conv}(x)\): Convolution operation applied to the input \(x\).
- \(x\): Input data for the convolution operation.

***Fine-Tuning***

During fine-tuning, only the fully connected layers are retrained to refine the model with minimal adjustments. This feedback loop, driven by Shapley values, enhances the recognition accuracy for similar gestures by focusing the model on the most critical data segments.

<!-- **Paper Summary**
The paper I used to incorporate the shapley value method was this [paper](https://www.sciencedirect.com/science/article/pii/S0020025524005802#fm0050). It introduces a method leveraging Shapley values for understanding and enhancing gesture recognition from sEMG signals. By treating each electrode channel as a "player" in a cooperative game, the Shapley value (ϕn​) quantifies each channel's contribution to recognizing gestures. The total contribution of all channels (CC) is calculated as the difference in model output with and without all input channels: 
C = f(all channels) - f(∅):
C: Total contribution of all electrode channels in gesture recognition.
f(all channels): Model output when all channels' signals are used as input.
f(∅) Model output when no channel signal is used as input (zero input).
The Shapley value for a channel n is computed by averaging its marginal contribution across all possible coalitions 
S⊆N: φ_n = Σ [S ⊆ (N \ {n})] [(|S|! * (|N| - |S| - 1)!) / |N|!] * [f(S ∪ {n}) - f(S)]. 
ϕ_n​: Shapley value of the nn-th channel, indicating its contribution to gesture recognition.
N: Total set of electrode channels.
S: Subset of channels excluding channel nn.
∣S∣: Number of channels in subset SS.
f(S∪{n}): Model output when subset SS plus channel nn is used as input.
f(S): Model output when only subset SS is used as input.
These values are aggregated to form a matrix across gestures, enabling a global analysis of each channel's importance.

***Enhancing Gesture Recognition Using Shapley Feedback:***
Shapley values guide model improvement by computing a contribution ratio (w_n) for each channel, normalized over gestures: 
w_n = φ_n / Σ_k φ_k. 
wn​: Contribution ratio of the nn-th channel, normalized Shapley value.
ϕn​: Shapley value of the nn-th channel.
∑kϕk​: Sum of all Shapley values for all channels.
This ratio is integrated into the first convolutional layer of the DNN, weighting the convolutional kernel to prioritize important channels: 
y = w_n * Conv(x). 
y: Weighted output after applying the convolution operation.
wn​: Contribution ratio of the nn-th channel.
Conv(x): Convolution operation applied to the input xx.
x: Input data for the convolution operation.
During fine-tuning, only the fully connected layers are retrained to refine the model with minimal adjustments. This feedback loop, driven by Shapley values, enhances the recognition accuracy for similar gestures by focusing the model on the most critical data segments. -->


***Network Adaptation:***

The SpectrogramConvNetWithShap is an adaptation of the original SpectrogramConvNet that integrates SHAP (SHapley Additive exPlanations) contributions to improve gesture recognition. The integration emphasizes the most important features during training.

SHAP Contribution Computation:

[SHAP contributions](https://shap.readthedocs.io/en/latest/) were precomputed on the test data, achieving a high accuracy of 96% per session. The SHAP outputs were shaped as (128, 4, 8, 10, 22), representing batch size, time, channels, frequency, and gestures. Positive SHAP contributions (weights) were extracted from this tensor to highlight the importance of specific input features for each gesture.

Dynamic SHAP Weight Application:

The extracted SHAP weights were dynamically applied during training. The model was modified to accept SHAP weights, which were normalized and aligned with the input spectrogram’s channel dimension. The blending of SHAP-weighted and unweighted inputs was controlled using the alpha blending equation:

x_shap = (1 - α) * x + α * (x * SHAP_weights)

Here:

x_shap​: SHAP-enhanced input.

x: Original input spectrogram.
SHAP_weights: SHAP contributions for the corresponding input channels.

α: A blending factor that increases over time to gradually incorporate SHAP contributions.

The parameter α was defined to increase gradually over the course of training. This ensures a smooth transition from unweighted inputs (α=0) to fully SHAP-weighted inputs (α=1).

Training with Pre-Trained Features:

The first two convolutional layers were frozen to retain pre-trained features, while the remaining layers were fine-tuned with SHAP-enhanced inputs. This approach preserved general feature extraction capabilities while leveraging SHAP contributions for task-specific optimization.

## Gesture specific results
Compare sessions to see how they improved with the new model(bar graph with session accuracy)

DANN Integration:

The DANN model incorporates SHAP (SHapley Additive exPlanations) contributions to enhance gesture classification across source (labeled) and target (unlabeled) domains. SHAP values, precomputed for each session and domain, quantify the importance of input channels, dynamically shaping the training process. These contributions are used to compute a SHAP-weighted loss, emphasizing critical channels, and a novel regularization term that aligns model activations with SHAP-determined relevance. The total loss combines gesture classification, domain adaptation, and SHAP-based regularization to ensure interpretable learning. By adversarially training a domain discriminator, the model bridges feature gaps between source and target domains thus incresing the accuracy of the previous DANN model by 2%

SHAP Importance Values:

SHAP values are precomputed for each domain, participant, and session using [DeepExplainer](https://shap.readthedocs.io/en/latest/) similar to the method used for the base model descibed above.
SHAP contributions are structured as a tensor (channels,classes), normalized to emphasize the most critical features across gestures.


SHAP-Weighted Loss:

SHAP values directly influence the loss computation, dynamically weighting input channels based on their importance. The weighted loss ensures that the model prioritizes more influential channels:

weighted_loss = ce_loss * channel_weights
Here:
weighted_loss: Loss function incorporating SHAP channel weights.

ce_loss: Cross-entropy loss between predicted and ground-truth labels.

channel_weights: SHAP importance values for each channel.

For gesture classification (lossgesture​) and domain discrimination (lossdomain​), SHAP values adaptively shape the learning process.

Regularization Term:

To align the model’s activations with SHAP importance, a regularization term is introduced. This term penalizes deviations from SHAP-guided feature contributions:

reg_term = Σ(activations * (1 - channel_weights))

Where:
activations: Summed feature activations across time and frequency dimensions.

channel_weights: SHAP-based channel importance scores.

The regularization ensures that feature activations reflect SHAP-determined importance, fostering a consistent mapping of input relevance to learned representations.


**Results after adaptaion**
Show improved graphs and tables maybe session graphs and tables

## Conclusion and Future Work

&nbsp;
<div align="center"><h4> <a href="https://github.com/roy2909/XAI-for-Hand-Gesture-Recognition">View Project on Github</a></h4></div>





